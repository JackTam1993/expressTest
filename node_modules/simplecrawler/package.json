{
  "_args": [
    [
      {
        "raw": "simplecrawler@1.1.5",
        "scope": null,
        "escapedName": "simplecrawler",
        "name": "simplecrawler",
        "rawSpec": "1.1.5",
        "spec": "1.1.5",
        "type": "version"
      },
      "/Users/tanjiajie/Desktop/js/expressTest"
    ]
  ],
  "_from": "simplecrawler@1.1.5",
  "_id": "simplecrawler@1.1.5",
  "_inCache": true,
  "_location": "/simplecrawler",
  "_nodeVersion": "6.9.5",
  "_npmOperationalInternal": {
    "host": "s3://npm-registry-packages",
    "tmp": "tmp/simplecrawler-1.1.5.tgz_1502826851629_0.3914389743003994"
  },
  "_npmUser": {
    "name": "fredrikekelund",
    "email": "fredrik@fredrik.computer"
  },
  "_npmVersion": "5.3.0",
  "_phantomChildren": {
    "lodash": "4.17.4"
  },
  "_requested": {
    "raw": "simplecrawler@1.1.5",
    "scope": null,
    "escapedName": "simplecrawler",
    "name": "simplecrawler",
    "rawSpec": "1.1.5",
    "spec": "1.1.5",
    "type": "version"
  },
  "_requiredBy": [
    "#USER",
    "/"
  ],
  "_resolved": "https://registry.npmjs.org/simplecrawler/-/simplecrawler-1.1.5.tgz",
  "_shasum": "a224efcc62a5f4777fdb091317bcb3873c84ca45",
  "_shrinkwrap": null,
  "_spec": "simplecrawler@1.1.5",
  "_where": "/Users/tanjiajie/Desktop/js/expressTest",
  "author": {
    "name": "Christopher Giffard",
    "email": "christopher.giffard@cgiffard.com"
  },
  "bin": {
    "crawl": "./lib/cli.js"
  },
  "bugs": {
    "url": "https://github.com/cgiffard/node-simplecrawler/issues"
  },
  "dependencies": {
    "async": "^2.1.4",
    "iconv-lite": "^0.4.13",
    "robots-parser": "^1.0.0",
    "urijs": "^1.18.11"
  },
  "description": "Very straightforward, event driven web crawler. Features a flexible queue interface and a basic cache mechanism with extensible backend.",
  "devDependencies": {
    "chai": "^3.2.0",
    "eslint": "^2.0.0",
    "jsdoc": "^3.4.0",
    "mocha": "^3.0.0"
  },
  "directories": {},
  "dist": {
    "integrity": "sha512-cmaCwGChXsyM/jO8q/zTL3HG1mURVBdit1PRcsmj93ov8hPva9tIAB9EsLHJURyGViOw6Th09YS7EF1u6Ykj7A==",
    "shasum": "a224efcc62a5f4777fdb091317bcb3873c84ca45",
    "tarball": "https://registry.npmjs.org/simplecrawler/-/simplecrawler-1.1.5.tgz"
  },
  "engines": {
    "node": ">=0.10.0"
  },
  "files": [
    "lib"
  ],
  "gitHead": "9c70222799f106fc8ca5b50973fe563567d73fa6",
  "homepage": "https://github.com/cgiffard/node-simplecrawler",
  "keywords": [
    "simple",
    "crawler",
    "spider",
    "cache",
    "queue",
    "simplecrawler",
    "eventemitter"
  ],
  "license": "BSD-2-Clause",
  "main": "./lib/index.js",
  "maintainers": [
    {
      "name": "Christopher Giffard",
      "email": "christopher.giffard@cgiffard.com"
    },
    {
      "name": "Fredrik Ekelund",
      "email": "fredrik@fredrik.computer"
    }
  ],
  "name": "simplecrawler",
  "optionalDependencies": {},
  "readme": "# Simple web crawler for node.js\n\n[![NPM version](https://img.shields.io/npm/v/simplecrawler.svg)](https://www.npmjs.com/package/simplecrawler)\n[![Linux Build Status](https://img.shields.io/travis/cgiffard/node-simplecrawler/master.svg)](https://travis-ci.org/cgiffard/node-simplecrawler)\n[![Windows Build Status](https://img.shields.io/appveyor/ci/cgiffard/node-simplecrawler/master.svg?label=Windows%20build)](https://ci.appveyor.com/project/cgiffard/node-simplecrawler/branch/master)\n[![Dependency Status](https://img.shields.io/david/cgiffard/node-simplecrawler.svg)](https://david-dm.org/cgiffard/node-simplecrawler)\n[![devDependency Status](https://img.shields.io/david/dev/cgiffard/node-simplecrawler.svg)](https://david-dm.org/cgiffard/node-simplecrawler#info=devDependencies)\n\nsimplecrawler is designed to provide a basic, flexible and robust API for\ncrawling websites. It was written to archive, analyse, and search some\nvery large websites and has happily chewed through hundreds of thousands of\npages and written tens of gigabytes to disk without issue.\n\n## What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for auto-detecting linked resources - which you can\n  replace or augment\n* Automatically respects any robots.txt rules\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except\n  when discovering links)\n\n## Documentation\n\n- [Installation](#installation)\n- [Getting started](#getting-started)\n- [Events](#events)\n    - [A note about HTTP error conditions](#a-note-about-http-error-conditions)\n    - [Waiting for asynchronous event listeners](#waiting-for-asynchronous-event-listeners)\n- [Configuration](#configuration)\n- [Fetch conditions](#fetch-conditions)\n- [Download conditions](#download-conditions)\n- [The queue](#the-queue)\n    - [Manually adding to the queue](#manually-adding-to-the-queue)\n    - [Queue items](#queue-items)\n    - [Queue statistics and reporting](#queue-statistics-and-reporting)\n    - [Saving and reloading the queue (freeze/defrost)](#saving-and-reloading-the-queue-freezedefrost)\n- [Cookies](#cookies)\n    - [Cookie events](#cookie-events)\n- [Link Discovery](#link-discovery)\n- [FAQ/Troubleshooting](#faqtroubleshooting)\n- [Node Support Policy](#node-support-policy)\n- [Current Maintainers](#current-maintainers)\n- [Contributing](#contributing)\n- [Contributors](#contributors)\n- [License](#license)\n\n## Installation\n\n```sh\nnpm install --save simplecrawler\n```\n\n## Getting Started\n\nInitializing simplecrawler is a simple process. First, you require the module\nand instantiate it with a single argument. You then configure the properties you\nlike (eg. the request interval), register a few event listeners, and call the\nstart method. Let's walk through the process!\n\nAfter requiring the crawler, we create a new instance of it. We supply the\nconstructor with a URL that indicates which domain to crawl and which resource\nto fetch first.\n\n```js\nvar Crawler = require(\"simplecrawler\");\n\nvar crawler = new Crawler(\"http://www.example.com/\");\n```\n\nYou can initialize the crawler with or without the `new` operator. Being able to\nskip it comes in handy when you want to chain API calls.\n\n```js\nvar crawler = Crawler(\"http://www.example.com/\")\n    .on(\"fetchcomplete\", function () {\n        console.log(\"Fetched a resource!\")\n    });\n```\n\nBy default, the crawler will only fetch resources on the same domain as that in\nthe URL passed to the constructor. But this can be changed through the\n`crawler.domainWhitelist` property.\n\nNow, let's configure some more things before we start crawling. Of course,\nyou're probably wanting to ensure you don't take down your web server. Decrease\nthe concurrency from five simultaneous requests - and increase the request\ninterval from the default 250 ms like this:\n\n```js\ncrawler.interval = 10000; // Ten seconds\ncrawler.maxConcurrency = 3;\n```\n\nYou can also define a max depth for links to fetch:\n\n```js\ncrawler.maxDepth = 1; // Only first page is fetched (with linked CSS & images)\n// Or:\ncrawler.maxDepth = 2; // First page and discovered links from it are fetched\n// Or:\ncrawler.maxDepth = 3; // Etc.\n```\n\nFor a full list of configurable properties, see the\n[configuration section](#configuration).\n\nYou'll also need to set up event listeners for the [events](#events) you want to\nlisten to. `fetchcomplete` and `complete` are a good place to start.\n\n```js\ncrawler.on(\"fetchcomplete\", function(queueItem, responseBuffer, response) {\n    console.log(\"I just received %s (%d bytes)\", queueItem.url, responseBuffer.length);\n    console.log(\"It was a resource of type %s\", response.headers['content-type']);\n});\n```\n\nThen, when you're satisfied and ready to go, start the crawler! It'll run\nthrough its queue finding linked resources on the domain to download, until it\ncan't find any more.\n\n```js\ncrawler.start();\n```\n\n## Events\n\nsimplecrawler's API is event driven, and there are plenty of events emitted\nduring the different stages of the crawl. Arguments passed to events are written\nin parentheses.\n\n* `crawlstart` -\n    Fired when the crawl begins or is restarted.\n* `queueadd` (queueItem) -\n    Fired when a new item is added to the queue.\n* `queueduplicate` (URLData) -\n    Fired when an item cannot be added to the queue because it is already\n    present in the queue. Frequent firing of this event is normal and expected.\n* `queueerror` (error, URLData) -\n    Fired when an item cannot be added to the queue due to an error.\n* `robotstxterror` (error) -\n    Fired when robots.txt couldn't be fetched.\n* `invaliddomain` (queueItem) -\n    Fired when a resource wasn't queued because it had an invalid domain. See\n    `crawler.filterByDomain`, `crawler.ignoreWWWDomain`,\n    `crawler.scanSubdomains` and `crawler.domainWhitelist` for different ways to\n    configure which domains are considered valid.\n* `fetchdisallowed` (queueItem) -\n    Fired when a resource wasn't queued because of robots.txt rules. See\n    `respectRobotsTxt` option.\n* `fetchprevented` (queueItem) -\n    Fired when a resource wasn't queued because of a [fetch\n    condition](#fetch-conditions).\n* `fetchconditionerror` (queueItem, error) -\n    Fired when one of the fetch conditions returns an error. Provides the queue\n    item that was processed when the error was encountered as well as the error\n    itself.\n* `downloadconditionerror` (queueItem, error) -\n    Fired when one of the download conditions returns an error. Provides the\n    queue item that was processed when the error was encountered as well as the\n    error itself.\n* `fetchstart` (queueItem, requestOptions) -\n    Fired when an item is spooled for fetching. If your event handler is\n    synchronous, you can modify the crawler request options (including headers\n    and request method.)\n* `fetchheaders` (queueItem, responseObject) -\n    Fired when the headers for a resource are received from the server. The node\n    `http` response object is returned for your perusal.\n* `cookieerror` (queueItem, error, setCookieHeader) -\n    Fired when an error was caught trying to add a cookie to the cookie jar.\n* `fetchredirect` (oldQueueItem, redirectQueueItem, responseObject) -\n    Fired when a redirect header is encountered. The new URL is processed and\n    passed as `redirectQueueItem`.\n* `fetch404` (queueItem, responseObject) -\n    Fired when a 404 HTTP status code is returned for a request.\n* `fetch410` (queueItem, responseObject) -\n    Fired when a 410 HTTP status code is returned for a request.\n* `fetchdataerror` (queueItem, responseObject) -\n    Fired when a resource can't be downloaded, because it exceeds the maximum\n    size we're prepared to receive (16MB by default.)\n* `fetchtimeout` (queueItem, crawlerTimeoutValue) -\n    Fired when a request time exceeds the internal crawler threshold.\n* `fetchcomplete` (queueItem, responseBody, responseObject) -\n    Fired after a resource has been completely downloaded and the server\n    returned an HTTP status code between 200 and 300. The response body is\n    provided as a Buffer per default, unless `decodeResponses` is truthy, in\n    which case it's a decoded string representation of the body.\n* `fetcherror` (queueItem, responseObject) -\n    Fired when an alternate 400 or 500 series HTTP status code is returned for a\n    request.\n* `gziperror` (queueItem, error, responseBuffer) -\n    Fired when a gzipped resource cannot be unzipped.\n* `fetchclienterror` (queueItem, error) -\n    Fired when a request dies locally for some reason. The error data is\n    returned as the second parameter.\n* `discoverycomplete` (queueItem, resources) -\n    Fired when linked resources have been discovered. Passes an array of\n    resources (as URL's) as the second parameter.\n* `complete` -\n    Fired when the crawler completes processing all the items in its queue, and\n    does not find any more to add. This event returns no arguments.\n\n### A note about HTTP error conditions\n\nBy default, simplecrawler does not download the response body when it encounters\nan HTTP error status in the response. If you need this information, you can listen\nto simplecrawler's error events, and through node's native `data` event\n(`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\n### Waiting for asynchronous event listeners\n\nSometimes, you might want to wait for simplecrawler to wait for you while you\nperform some asynchronous tasks in an event listener, instead of having it\nracing off and firing the `complete` event, halting your crawl. For example,\nif you're doing your own link discovery using an asynchronous library method.\n\nsimplecrawler provides a `wait` method you can call at any time. It is available\nvia `this` from inside listeners, and on the crawler object itself. It returns\na callback function.\n\nOnce you've called this method, simplecrawler will not fire the `complete` event\nuntil either you execute the callback it returns, or a timeout is reached\n(configured in `crawler.listenerTTL`, by default 10000 ms.)\n\n#### Example asynchronous event listener\n\n```js\ncrawler.on(\"fetchcomplete\", function(queueItem, data, res) {\n    var continue = this.wait();\n    doSomeDiscovery(data, function(foundURLs) {\n        foundURLs.forEach(crawler.queueURL.bind(crawler));\n        continue();\n    });\n});\n```\n\n## Configuration\n\nsimplecrawler is highly configurable and there's a long list of settings you can\nchange to adapt it to your specific needs.\n\n* `crawler.host` -\n    The domain to scan. By default, simplecrawler will restrict all requests to\n    this domain.\n* `crawler.interval=250` -\n    The interval with which the crawler will spool up new requests (one per\n    tick).\n* `crawler.maxConcurrency=5` -\n    The maximum number of requests the crawler will run simultaneously. Defaults\n    to 5 - the default number of http agents node will run.\n* `crawler.timeout=300000` -\n    The maximum time in milliseconds the crawler will wait for headers before\n    aborting the request.\n* `crawler.listenerTTL=10000` -\n    The maximum time in milliseconds the crawler will wait for async listeners.\n* `crawler.userAgent=\"Node/simplecrawler <version> (https://github.com/cgiffard/node-simplecrawler)\"` -\n    The user agent the crawler will report.\n* `crawler.decompressResponses=true` -\n    Response bodies that are compressed will be automatically decompressed\n    before they're emitted in the `fetchcomplete` event. Even if this is falsy,\n    compressed responses will be decompressed before they're passed to the\n    `discoverResources` method.\n* `crawler.decodeResponses=false` -\n    Response bodies will be intelligently character converted to standard\n    JavaScript strings using the\n    [iconv-lite](https://www.npmjs.com/package/iconv-lite) module. The character\n    encoding is interpreted from the Content-Type header firstly, and secondly\n    from any `<meta charset=\"xxx\" />` tags.\n* `crawler.respectRobotsTxt=true` -\n    Controls whether the crawler should respect rules in robots.txt (if such a\n    file is present). The\n    [robots-parser](https://www.npmjs.com/package/robots-parser) module is used\n    to do the actual parsing. This property will also make the default\n    `crawler.discoverResources` method respect\n    `<meta name=\"robots\" value=\"nofollow\">` tags - meaning that no resources\n    will be extracted from pages that include such a tag.\n* `crawler.queue` -\n    The queue in use by the crawler (Must implement the `FetchQueue` interface)\n* `crawler.allowInitialDomainChange=false` -\n    If the response for the initial URL is a redirect to another domain (e.g.\n    from github.net to github.com), update `crawler.host` to continue the\n    crawling on that domain.\n* `crawler.filterByDomain=true` -\n    Specifies whether the crawler will restrict queued requests to a given\n    domain/domains.\n* `crawler.scanSubdomains=false` -\n    Enables scanning subdomains (other than www) as well as the specified\n    domain.\n* `crawler.ignoreWWWDomain=true` -\n    Treats the `www` domain the same as the originally specified domain.\n* `crawler.stripWWWDomain=false` -\n    Or go even further and strip WWW subdomain from requests altogether!\n* `crawler.stripQuerystring=false` -\n    Specify to strip querystring parameters from URL's.\n* `crawler.sortQueryParameters=false` -\n    Specify to sort the querystring parameters before queueing URL's. This is\n    to canonicalize URLs so that foo?a=1&b=2 is considered same as foo?b=2&a=1.\n* `crawler.discoverResources` -\n    simplecrawler's default resource discovery function -\n    which, given a buffer containing a resource, returns an array of URLs.\n    For more details about link discovery, see [Link Discovery](#link-discovery)\n* `crawler.discoverRegex` -\n    Array of regular expressions and functions that simplecrawler uses to\n    discover resources. Functions in this array are expected to return an array.\n    *Only applicable if the default `discoverResources` function is used.*\n* `crawler.parseHTMLComments=true` -\n    Whether to scan for URL's inside HTML comments. *Only applicable if the\n    default `discoverResources` function is used.*\n* `crawler.parseScriptTags=true` -\n    Whether to scan for URL's inside script tags. *Only applicable if the\n    default `discoverResources` function is used.*\n* `crawler.cache` -\n    Specify a cache architecture to use when crawling. Must implement\n    `SimpleCache` interface. You can save the site to disk using the built in\n    file system cache like this:\n\n    ```js\n    crawler.cache = new Crawler.cache('pathToCacheDirectory');\n    ```\n\n* `crawler.useProxy=false` -\n    The crawler should use an HTTP proxy to make its requests.\n* `crawler.proxyHostname=\"127.0.0.1\"` -\n    The hostname of the proxy to use for requests.\n* `crawler.proxyPort=8123` -\n    The port of the proxy to use for requests.\n* `crawler.proxyUser=null` -\n    The username for HTTP/Basic proxy authentication (leave unset for\n    unauthenticated proxies.)\n* `crawler.proxyPass=null` -\n    The password for HTTP/Basic proxy authentication (leave unset for\n    unauthenticated proxies.)\n* `crawler.domainWhitelist` -\n    An array of domains the crawler is permitted to crawl from. If other\n    settings are more permissive, they will override this setting.\n* `crawler.allowedProtocols` -\n    An array of RegExp objects used to determine whether a URL protocol is\n    supported. This is to deal with nonstandard protocol handlers that regular\n    HTTP is sometimes given, like `feed:`. It does not provide support for\n    non-http protocols (and why would it!?)\n* `crawler.maxResourceSize=16777216` -\n    The maximum resource size that will be downloaded, in bytes. Defaults to\n    16MB.\n* `crawler.supportedMimeTypes` -\n    An array of RegExp objects and/or strings used to determine what MIME types\n    simplecrawler should look for resources in. If `crawler.downloadUnsupported`\n    is false, this also restricts what resources are downloaded.\n* `crawler.downloadUnsupported=true` -\n    simplecrawler will download files it can't parse (determined by\n    `crawler.supportedMimeTypes`). Defaults to true, but if you'd rather save\n    the RAM and GC lag, switch it off. When false, it closes sockets for\n    unsupported resources.\n* `crawler.needsAuth=false` -\n    Flag to specify if the domain you are hitting requires basic authentication.\n* `crawler.authUser=\"\"` -\n    Username provided for `needsAuth` flag.\n* `crawler.authPass=\"\"` -\n    Password provided for `needsAuth` flag.\n* `crawler.customHeaders` -\n    An object specifying a number of custom headers simplecrawler will add to\n    every request. These override the default headers simplecrawler sets, so be\n    careful with them. If you want to tamper with headers on a per-request\n    basis, see the `fetchqueue` event.\n* `crawler.acceptCookies=true` -\n    Flag to indicate if the crawler should hold on to cookies.\n* `crawler.urlEncoding=\"unicode\"` -\n    Set this to `iso8859` to trigger\n    [URI.js](https://medialize.github.io/URI.js/)' re-encoding of iso8859 URL's\n    to unicode.\n* `crawler.maxDepth=0` -\n    Defines a maximum distance from the original request at which resources will\n    be downloaded.\n* `crawler.ignoreInvalidSSL=false` -\n    Treat self-signed SSL certificates as valid. SSL certificates will not be\n    validated against known CAs. Only applies to https requests. You may also\n    have to set the environment variable NODE_TLS_REJECT_UNAUTHORIZED to '0'.\n    For example: `process.env.NODE_TLS_REJECT_UNAUTHORIZED = '0';`\n\n## Fetch conditions\n\nsimplecrawler has an concept called fetch conditions that offers a flexible API\nfor filtering discovered resources before they're put in the queue. A fetch\ncondition is a function that takes a queue item candidate and evaluates\n(synchronously or asynchronously) whether it should be added to the queue or\nnot. *Please note: with the next major release, all fetch conditions will be\nasynchronous.*\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nsimplecrawler will evaluate every fetch condition in parallel until one is\nencountered that returns a falsy value. If that happens, the resource in\nquestion will not be fetched.\n\nThis API is complemented by [download conditions](#download-conditions) that\ndetermine whether a resource's body data should be downloaded.\n\n### Adding a fetch condition\n\nThis example fetch condition prevents URL's ending in `.pdf` from being\ndownloaded. Adding a fetch condition assigns it an ID, which the\n`addFetchCondition` function returns. You can use this ID to remove the\ncondition later.\n\n```js\nvar conditionID = myCrawler.addFetchCondition(function(queueItem, referrerQueueItem, callback) {\n    callback(null, !queueItem.path.match(/\\.pdf$/i));\n});\n```\n\nFetch conditions are called with three arguments: `queueItem`,\n`referrerQueueItem` and `callback`. `queueItem` represents the resource to be\nfetched (or not), and `referrerQueueItem` represents the resource where the new\n`queueItem` was discovered. See the [queue item documentation](#queue-items) for\ndetails on their structure. The `callback` argument is optional, but if your\nfunction takes 3 arguments, simplecrawler will consider it asynchronous and wait\nfor the `callback` to be called. If your function takes 2 arguments or less,\nsimplecrawler will consider it synchronous and look at its return value instead.\n**Please note** however, that this flexibility in sync and async behavior is due\nto change with the next major release when all fetch conditions will need to use\nthe asynchronous API.\n\nWith this information, you can write sophisticated logic for determining which\npages to fetch and which to avoid. For example, you could write a program that\nensures all links on a website - both internal and external - return good HTTP\nstatuses. Here's an example:\n\n```js\nvar crawler = new Crawler(\"http://example.com\");\ncrawler.filterByDomain = false;\n\ncrawler.addFetchCondition(function(queueItem, referrerQueueItem, callback) {\n    // We only ever want to move one step away from example.com, so if the\n    // referrer queue item reports a different domain, don't proceed\n    callback(null, referrerQueueItem.host === crawler.host);\n});\n\ncrawler.start();\n```\n\n### Removing a fetch condition\n\nWith the ID of the fetch condition you added earlier, or with a reference to the\ncalback function you registered, you can remove the fetch condition using the\n`crawler.removeFetchCondition` method:\n\n```js\nfunction listener(queueItem, stateData) {\n    // Do something\n}\n\nvar conditionID = myCrawler.addFetchCondition(listener);\n\n// By id...\nmyCrawler.removeFetchCondition(conditionID);\n// or by reference\nmyCrawler.removeFetchCondition(listener);\n```\n\n## Download conditions\n\nWhile fetch conditions let you determine which resources to put in the queue,\ndownload conditions offer the same kind of flexible API for determining which\nresources' data to download. Download conditions support both a synchronous and\nan asynchronous API, but *with the next major release, all download conditions\nwill be asynchronous.*\n\nDownload conditions are evaluated after the headers of a resource have been\ndownloaded, if that resource returned an HTTP status between 200 and 299. This\nlets you inspect the content-type and content-length headers, along with all\nother properties on the queue item, before deciding if you want this resource's\ndata or not.\n\n### Adding a download condition\n\nDownload conditions are added in much the same way as fetch conditions, with the\n`crawler.addDownloadCondition` method. This method returns an ID that can be\nused to remove the condition later.\n\n```js\nvar conditionID = myCrawler.addDownloadCondition(function(queueItem, response, callback) {\n    callback(null,\n        queueItem.stateData.contentType === \"image/png\" &&\n        queueItem.stateData.contentLength < 5 * 1000 * 1000\n    );\n});\n```\n\nDownload conditions are called with three arguments: `queueItem`, `response` and\n`callback`. `queueItem` represents the resource that's being fetched ([queue\nitem structure](#queue-items)) and `response` is an instance of\n`http.IncomingMessage`. Please see the [node\ndocumentation](https://nodejs.org/api/http.html#http_class_http_incomingmessage)\nfor that class for more details on what it looks like.\n\n### Removing a download condition\n\nJust like with fetch conditions, download conditions can be removed with the ID\nreturned from the `addDownloadCondition` method, or with a reference to the same\ncallback function. `crawler.removeDownloadCondition` is the method you'll use:\n\n```js\nfunction listener(queueItem, response, callback) {\n    // Do something\n}\n\nvar conditionID = myCrawler.addDownloadCondition(listener);\n\n// By id...\nmyCrawler.removeDownloadCondition(conditionID);\n// or by reference\nmyCrawler.removeDownloadCondition(listener);\n```\n\n## The queue\n\nLike any other web crawler, simplecrawler has a queue. It can be directly\naccessed through `crawler.queue` and implements an asynchronous interface for\naccessing queue items and statistics. There are several methods for interacting\nwith the queue, the simplest being `crawler.queue.get`, which lets you get a\nqueue item at a specific index in the queue.\n\n```js\ncrawler.queue.get(5, function (queueItem) {\n    // Do something with the queueItem\n});\n```\n\n*All queue method are in reality synchronous by default, but simplecrawler is\nbuilt to be able to use different queues that implement the same interface, and\nthose implementations can be asynchronous - which means they could eg. be backed\nby a database.*\n\n### Manually adding to the queue\n\nTo add items to the queue, use `crawler.queueURL`. This method takes 3\narguments: a URL to queue, a referrer queue item and a boolean that indicates\nwhether the URL should be queued regardless of whether it already exists in the\nqueue or not.\n\n```js\ncrawler.queueURL(\"/example.html\", referrerQueueItem, false);\n```\n\n### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `id` - A unique ID assigned by the queue when the queue item is added\n* `url` - The complete, canonical URL of the resource\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The URL path, including the query string\n* `uriPath` - The URL path, excluding the query string\n* `depth` - How many steps simplecrawler has taken from the initial page (which\nis depth 1) to this resource.\n* `fetched` - Has the request for this item been completed? You can monitor this\nas requests are processed.\n* `status` - The internal status of the item, always a string. This can be one\nof:\n    * `\"queued\"` - The resource is in the queue to be fetched, but nothing's\n    happened to it yet.\n    * `\"spooled\"` - A request has been made to the remote server, but we're\n    still waiting for a response.\n    * `\"headers\"` - The headers for the resource have been received.\n    * `\"downloaded\"` - The item has been entirely downloaded.\n    * `\"redirected\"` - The resource request returned a 300 series response, with\n    a Location header and a new URL.\n    * `\"notfound\"` - The resource could not be found, ie. returned a 404 or 410\n    HTTP status.\n    * `\"failed\"` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the\nrequest:\n    * `requestLatency` - The time taken for headers to be received after the\n    request was made.\n    * `requestTime` - The total time taken for the request (including download\n    time.)\n    * `downloadTime` - The total time taken for the resource to be downloaded.\n    * `contentLength` - The length (in bytes) of the returned content.\n    Calculated based on the `content-length` header.\n    * `contentType` - The MIME type of the content.\n    * `code` - The HTTP status code returned for the request. Note that this\n      code is `600` if an error occurred in the client and a fetch operation\n      could not take place successfully.\n    * `headers` - An object containing the header information returned by the\n    server. This is the object node returns as part of the `response` object.\n    * `actualDataSize` - The length (in bytes) of the returned content.\n    Calculated based on what is actually received, not the `content-length`\n    header.\n    * `sentIncorrectSize` - True if the data length returned by the server did\n    not match what we were told to expect by the `content-length` header.\n\nAs you can see, you can get a lot of meta-information out about each request.\nThis has been put to use by providing some convenient methods for getting simple\naggregate data about the queue.\n\n### Queue statistics and reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl so far. This is done live, so don't check it 30 times\na second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nYou can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively.\n\n```js\ncrawler.queue.max(\"requestLatency\", function(error, max) {\n    console.log(\"The maximum request latency was %dms.\", max);\n});\ncrawler.queue.min(\"downloadTime\", function(error, min) {\n    console.log(\"The minimum download time was %dms.\", min);\n});\ncrawler.queue.avg(\"actualDataSize\", function(error, avg) {\n    console.log(\"The average resource size received is %d bytes.\", avg);\n});\n```\n\nFor general filtering or counting of queue items, there are two methods:\n`crawler.queue.filterItems` and `crawler.queue.countItems`. Both take an object\ncomparator and a callback.\n\n```js\ncrawler.queue.countItems({ fetched: true }, function(error, count) {\n    console.log(\"The number of completed items is %d\", count);\n});\n\ncrawler.queue.filterItems({ status: \"notfound\" }, function(error, items) {\n    console.log(\"These items returned 404 or 410 HTTP statuses\", items);\n});\n```\n\nThe object comparator can also contain other objects, so you may filter queue\nitems based on properties in their `stateData` object as well.\n\n```js\ncrawler.queue.filterItems({\n    stateData: { code: 301 }\n}, function(error, items) {\n    console.log(\"These items returned a 301 HTTP status\", items);\n});\n```\n\n### Saving and reloading the queue (freeze/defrost)\n\nIt can be convenient to be able to save the crawl progress and later be able to\nreload it if your application fails or you need to abort the crawl for some\nreason. The `crawler.queue.freeze` and `crawler.queue.defrost` methods will let\nyou do this.\n\n**A word of warning** - they are not CPU friendly as they rely on `JSON.parse`\nand `JSON.stringify`. Use them only when you need to save the queue - don't call\nthem after every request or your application's performance will be incredibly\npoor - they block like *crazy*. That said, using them when your crawler\ncommences and stops is perfectly reasonable.\n\nNote that the methods themselves are asynchronous, so if you are going to exit\nthe process after you do the freezing, make sure you wait for callback -\notherwise you'll get an empty file.\n\n```js\ncrawler.queue.freeze(\"mysavedqueue.json\", function () {\n    process.exit();\n});\n\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Cookies\n\nsimplecrawler has an internal cookie jar, which collects and resends cookies\nautomatically and by default. If you want to turn this off, set the\n`crawler.acceptCookies` option to `false`. The cookie jar is accessible via\n`crawler.cookies`, and is an event emitter itself.\n\n### Cookie events\n\n* `addcookie` (cookie) - Fired when a new cookie is added to the jar.\n* `removecookie` (cookie array) - Fired when one or more cookies are removed from the jar.\n\n## Link Discovery\n\nsimplecrawler's discovery function is made to be replaceable — you can\neasily write your own that discovers only the links you're interested in.\n\nThe method must accept a buffer and a [`queueItem`](#queue-items), and\nreturn the resources that are to be added to the queue.\n\nIt is quite common to pair simplecrawler with a module like\n[cheerio](https://npmjs.com/package/cheerio) that can correctly parse\nHTML and provide a DOM like API for querying — or even a whole headless\nbrowser, like phantomJS.\n\nThe example below demonstrates how one might achieve basic HTML-correct\ndiscovery of only link tags using cheerio.\n\n```js\ncrawler.discoverResources = function(buffer, queueItem) {\n    var $ = cheerio.load(buffer.toString(\"utf8\"));\n\n    return $(\"a[href]\").map(function () {\n        return $(this).attr(\"href\");\n    }).get();\n};\n```\n\n## FAQ/Troubleshooting\n\nThere are a couple of questions that pop up more often than others in the issue\ntracker. If you're having trouble with simplecrawler, please have a look at the\nlist below before submitting an issue.\n\n- **Q: Why does simplecrawler discover so many invalid URLs?**\n\n    A: simplecrawler's built-in discovery method is purposefully naive - it's a\n    brute force approach intended to find everything: URLs in comments, binary files,\n    scripts, image EXIF data, inside CSS documents, and more — useful for archiving\n    and use cases where it's better to have false positives than fail to discover a\n    resource.\n\n    It's definitely not a solution for every case, though — if you're\n    writing a link checker or validator, you don't want erroneous 404s\n    throwing errors. Therefore, simplecrawler allows you to tune discovery in a few\n    key ways:\n\n    - You can either add to (or remove from) the `discoverRegex` array, tweaking\n      the search patterns to meet your requirements; or\n    - Swap out the `discoverResources` method. Parsing HTML pages is beyond the\n      scope of simplecrawler, but it is very common to combine simplecrawler with\n      a module like [cheerio](https://npmjs.com/package/cheerio) for more\n      sophisticated resource discovery.\n\n    Further documentation is available in the [link discovery](#link-discovery)\n    section.\n\n- **Q: Why did simplecrawler complete without fetching any resources?**\n\n    A: When this happens, it is usually because the initial request was redirected\n    to a different domain that wasn't in the `domainWhitelist`.\n\n- **Q: How do I crawl a site that requires a login?**\n\n    A: Logging in to a site is usually fairly simple and most login procedures\n    look alike. We've included an example that covers a lot of situations, but\n    sadly, there isn't a one true solution for how to deal with logins, so\n    there's no guarantee that this code works right off the bat.\n\n    What we do here is:\n    1. fetch the login page,\n    2. store the session cookie assigned to us by the server,\n    3. extract any CSRF tokens or similar parameters required when logging in,\n    4. submit the login credentials.\n\n    ```js\n    var Crawler = require(\"simplecrawler\"),\n        url = require(\"url\"),\n        cheerio = require(\"cheerio\"),\n        request = require(\"request\");\n\n    var initialURL = \"https://example.com/\";\n\n    var crawler = new Crawler(initialURL);\n\n    request(\"https://example.com/login\", {\n        // The jar option isn't necessary for simplecrawler integration, but it's\n        // the easiest way to have request remember the session cookie between this\n        // request and the next\n        jar: true\n    }, function (error, response, body) {\n        // Start by saving the cookies. We'll likely be assigned a session cookie\n        // straight off the bat, and then the server will remember the fact that\n        // this session is logged in as user \"iamauser\" after we've successfully\n        // logged in\n        crawler.cookies.addFromHeaders(response.headers[\"set-cookie\"]);\n\n        // We want to get the names and values of all relevant inputs on the page,\n        // so that any CSRF tokens or similar things are included in the POST\n        // request\n        var $ = cheerio.load(body),\n            formDefaults = {},\n            // You should adapt these selectors so that they target the\n            // appropriate form and inputs\n            formAction = $(\"#login\").attr(\"action\"),\n            loginInputs = $(\"input\");\n\n        // We loop over the input elements and extract their names and values so\n        // that we can include them in the login POST request\n        loginInputs.each(function(i, input) {\n            var inputName = $(input).attr(\"name\"),\n                inputValue = $(input).val();\n\n            formDefaults[inputName] = inputValue;\n        });\n\n        // Time for the login request!\n        request.post(url.resolve(initialURL, formAction), {\n            // We can't be sure that all of the input fields have a correct default\n            // value. Maybe the user has to tick a checkbox or something similar in\n            // order to log in. This is something you have to find this out manually\n            // by logging in to the site in your browser and inspecting in the\n            // network panel of your favorite dev tools what parameters are included\n            // in the request.\n            form: Object.assign(formDefaults, {\n                username: \"iamauser\",\n                password: \"supersecretpw\"\n            }),\n            // We want to include the saved cookies from the last request in this\n            // one as well\n            jar: true\n        }, function (error, response, body) {\n            // That should do it! We're now ready to start the crawler\n            crawler.start();\n        });\n    });\n\n    crawler.on(\"fetchcomplete\", function (queueItem, responseBuffer, response) {\n        console.log(\"Fetched\", queueItem.url, responseBuffer.toString());\n    });\n    ```\n\n- **Q: What does it mean that events are asynchronous?**\n\n    A: One of the core concepts of node.js is its asynchronous nature. I/O\n    operations (like network requests) take place outside of the main thread\n    (which is where your code is executed). This is what makes node fast, the\n    fact that it can continue executing code while there are multiple HTTP\n    requests in flight, for example. But to be able to get back the result of\n    the HTTP request, we need to register a function that will be called when\n    the result is ready. This is what *asynchronous* means in node - the fact\n    that code can continue executing while I/O operations are in progress - and\n    it's the same concept as with AJAX requests in the browser.\n\n- **Q: Promises are nice, can I use them with simplecrawler?**\n\n    A: No, not really. Promises are meant as a replacement for callbacks, but\n    simplecrawler is event driven, not callback driven. Using callbacks to any\n    greater extent in simplecrawler wouldn't make much sense, since you normally\n    need to react more than once to what happens in simplecrawler.\n\n- **Q: Something's happening and I don't see the output I'm expecting!**\n\n    Before filing an issue, check to see that you're not just missing something by\n    logging *all* crawler events with the code below:\n\n    ```js\n    var originalEmit = crawler.emit;\n    crawler.emit = function(evtName, queueItem) {\n        crawler.queue.countItems({ fetched: true }, function(err, completeCount) {\n            if (err) {\n                throw err;\n            }\n\n            crawler.queue.getLength(function(err, length) {\n                if (err) {\n                    throw err;\n                }\n\n                console.log(\"fetched %d of %d — %d open requests, %d open listeners\",\n                    completeCount,\n                    length,\n                    crawler._openRequests.length,\n                    crawler._openListeners);\n            });\n        });\n\n        console.log(evtName, queueItem ? queueItem.url ? queueItem.url : queueItem : null);\n        originalEmit.apply(crawler, arguments);\n    };\n    ```\n\n    If you don't see what you need after inserting that code block, and you still need help,\n    please attach the output of all the events fired with your email/issue.\n\n## Node Support Policy\n\nSimplecrawler will officially support stable and LTS versions of Node which are\ncurrently supported by the Node Foundation.\n\nCurrently supported versions:\n\n- 4.x\n- 5.x\n- 6.x\n- 7.x\n\n## Current Maintainers\n\n* [Christopher Giffard](https://github.com/cgiffard)\n* [Fredrik Ekelund](https://github.com/fredrikekelund)\n* [XhmikosR](https://github.com/XhmikosR)\n\n## Contributing\n\nPlease see the [contributor guidelines](https://github.com/cgiffard/node-simplecrawler/blob/master/CONTRIBUTING.md)\nbefore submitting a pull request to ensure that your contribution is able to be\naccepted quickly and easily!\n\n## Contributors\n\nsimplecrawler has benefited from the kind efforts of dozens of contributors, to\nwhom we are incredibly grateful. We originally listed their individual\ncontributions but it became pretty unwieldy - the\n[full list can be found here.](https://github.com/cgiffard/node-simplecrawler/graphs/contributors)\n\n## License\n\nCopyright (c) 2016, Christopher Giffard.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
  "readmeFilename": "README.md",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/cgiffard/node-simplecrawler.git"
  },
  "scripts": {
    "docs": "jsdoc -c jsdoc.json",
    "lint": "eslint example/ lib/ test/",
    "mocha": "mocha -R spec -t 5000",
    "test": "npm run lint && npm run mocha"
  },
  "version": "1.1.5"
}
